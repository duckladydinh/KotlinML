package thuan.handsome.lbfgsb

/**
 * High-level wrapper for the L-BFGS-B algorithm
 */
class LBFGSBOptimizer {
	companion object {
		/**
		 * @param func - a differential function
		 *
		 * @param xZero - an initial guess
		 *
		 * @param bounds - an array of Bounds which defines the limits of each variable.
		 * Null value is used for no limit such as Bound(1, null) means no upper limit.
		 *
		 * @param maxGradientNorm - (pgtol) maximal acceptable gradient value
		 * The iteration will stop when ``max{|proj g_i | i = 1, ..., n} <= pgtol``
		 * where $proj g_i$ is i-th component of the the projected gradient.
		 *
		 * @param maxCorrections - The maximum number of variable metric corrections used
		 * to define the limited memory matrix. (The limited memory BFGS method does
		 * not store the full hessian but uses this many terms in an approximation to it).
		 *
		 * According to the original fortran documentation, range [3, 20] is recommended.
		 *
		 * @param funcReductionFactor - (factr, ftol/eps) relative function reduction
		 * factor. The iteration will stop when
		 *              `(f^k - f^{k+1})/max{|f^k|,|f^{k+1}|,1} <= factr * eps``,
		 * where ``eps`` is the machine precision, which is automatically
		 * generate`(f^k - f^{k+1})/max{|f^k|,|f^{k+1}|,1} <= factr * eps``,
		 * where ``eps`` is the machine precision, which is automatically
		 * generated by the coded by the code.
		 *
		 * Example values for 15 digits accuracy:
		 *  * 1e+12 for low accuracy,
		 *  * 1e+7  for moderate accuracy,
		 *  * 1e+1  for extremely high accuracy
		 *
		 *  @param verbose -1 means no logging, increasing will increase logging
		 *
		 *  @param epsilon > 0 the delta x needed to compute empirical grads
		 *
		 *  @param callback (x, y, grads) - a method to call after each iteration
		 */

		fun minimize(
			// most important inputs
			func: (DoubleArray) -> Double,
			xZero: DoubleArray,
			bounds: List<Bound> = getUnbounds(xZero.size),
			// algorithm's parameters
			maxIterations: Int = 15000,
			maxCorrections: Int = 10,
			maxGradientNorm: Double = 1e-5,
			funcReductionFactor: Double = 1e7,
			// external parameters
			verbose: Int = -1,
			epsilon: Double = 1e-8,
			callback: ((DoubleArray, Double, DoubleArray) -> Boolean)? = null
		): Summary {
			val diffFunc = differentialFunctionOf(func, epsilon)
			assert(bounds.size == xZero.size) {
				"Bounds number (${bounds.size}) doesn't match starting point size (${xZero.size})"
			}

			val optimizer = LBFGSBWrapper(xZero.size, maxCorrections).apply {
				setX(xZero)
				setBounds(bounds)
				setDebugLevel(verbose)
				setMaxGradientNorm(maxGradientNorm)
				setFunctionFactor(funcReductionFactor)
			}

			val info = optimizer.minimize(diffFunc, maxIterations, callback)
			val summary = Summary(optimizer.getX(), optimizer.getY(), optimizer.getGrads(), info)

			optimizer.close()
			return summary
		}

		private fun differentialFunctionOf(
			func: (DoubleArray) -> Double, epsilon: Double = 1e-8
		): (DoubleArray) -> Pair<Double, DoubleArray> {
			return { x ->
				val y = func.invoke(x)
				val grads = gradientsOf(func, x, y, epsilon)
				Pair(y, grads)
			}
		}

		private fun gradientsOf(
			func: (DoubleArray) -> Double,
			xZero: DoubleArray,
			yZero: Double,
			epsilon: Double
		): DoubleArray {
			assert(epsilon > 0)

			return DoubleArray(xZero.size) {
				xZero[it] += epsilon
				val y = func.invoke(xZero)
				val gradient = (y - yZero) / epsilon
				xZero[it] -= epsilon

				gradient
			}
		}

		private fun getUnbounds(numVariables: Int): List<Bound> {
			return (1..numVariables).map { Bound(null, null) }.toList()
		}
	}
}
